<div align="center">

<img src="./assets/logo.png" width="400"/><br>


[![license](https://img.shields.io/github/license/InternLM/xtuner.svg)](./LICENSE)
[![xtuner](https://img.shields.io/badge/support-xtuner-blue)](https://github.com/InternLM/xtuner/)
[![lmdeploy](https://img.shields.io/badge/lmdeploy-blue)](https://github.com/InternLM/lmdeploy/)
[![sglang](https://img.shields.io/badge/sglang-blue)](https://github.com/sgl-project/sglang/)
[![vllm](https://img.shields.io/badge/vllm-blue)](https://github.com/vllm-project/vllm/)


[ğŸ¤— HuggingFace](https://huggingface.co/collections/internlm/polar-68693f829d2e83ac5e6e124a) |
[ğŸ¤– ModelScope](https://www.modelscope.cn/organization/Shanghai_AI_Laboratory) |
[ğŸ“œ Paper](https://arxiv.org/abs/2507.05197)<br>


[English](./README.md) |
[ç®€ä½“ä¸­æ–‡](./README_zh-CN.md)

</div>

# ç®€ä»‹

POLAR æ˜¯ä¸€ä¸ªç»è¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼Œåœ¨è®­ç»ƒèŒƒå¼å’Œæ¨¡å‹æ€§èƒ½ä¸Šå–å¾—äº†é‡å¤§çªç ´ã€‚æˆ‘ä»¬åˆ©ç”¨å…¨æ–°çš„ç­–ç•¥åˆ¤åˆ«å­¦ä¹ æ–¹æ³•ï¼ˆPolicy Discriminative Learningï¼ŒPOLARï¼‰ï¼Œä½¿ç”¨å¤§è§„æ¨¡åˆæˆè¯­æ–™è¿›è¡Œé«˜æ•ˆæ‰©å±•é¢„è®­ç»ƒï¼Œä½¿å¥–åŠ±æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†ä¸åŒçš„è¯­è¨€æ¨¡å‹å’Œç­–ç•¥åˆ†å¸ƒã€‚ç»è¿‡é¢„è®­ç»ƒçš„ POLAR å¯é€šè¿‡å°‘é‡çš„åå¥½æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä»¥å¿«é€Ÿå¯¹é½äººç±»åå¥½ã€‚POLAR çš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š

* **å…¨æ–°çš„é¢„è®­ç»ƒèŒƒå¼**ï¼šPOLAR è®©å¥–åŠ±æ¨¡å‹å­¦ä¼šè¯†åˆ«ç›¸åŒçš„ç­–ç•¥å¹¶åŒºåˆ†ä¸åŒçš„ç­–ç•¥ã€‚ä¸ä¼ ç»Ÿçš„ä¾èµ–ç»å¯¹åå¥½çš„å¥–åŠ±å»ºæ¨¡æ–¹æ³•ä¸åŒï¼ŒPOLAR èƒ½å¤Ÿå­¦ä¹ ä¸¤ä¸ªç­–ç•¥ä¹‹é—´çš„ç›¸å¯¹å·®å¼‚ï¼Œæ˜¯ä¸€ç§å¯æ‰©å±•çš„ã€é«˜å±‚æ¬¡çš„ä¼˜åŒ–ç›®æ ‡ã€‚

* **ä¸“ä¸ºå¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼ˆRFTï¼‰è®¾è®¡:**  POLAR æ ¹æ®ç»™å®šçš„å‚è€ƒç­”æ¡ˆä¸ºè¯­è¨€æ¨¡å‹çš„è¾“å‡ºæ‰“åˆ†ï¼Œå®Œç¾å¥‘åˆå¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼ˆRFTï¼‰æ¡†æ¶ï¼Œä¸ºå¼ºåŒ–å­¦ä¹ å¾®è°ƒåœ¨é€šç”¨åœºæ™¯çš„åº”ç”¨æä¾›äº†ä¸€ç§æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚

* **å“è¶Šçš„æ€§èƒ½ä¸æ³›åŒ–èƒ½åŠ›:** POLAR åœ¨ä¸‹æ¸¸å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­å±•ç°å‡ºé¢†å…ˆçš„æ°´å¹³ï¼Œå¯ç¨³å®šåœ°æä¾›å‡†ç¡®å¯é çš„å¥–åŠ±ä¿¡å·ã€‚POLAR å…·æœ‰æå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯æœ‰æ•ˆæ³›åŒ–åˆ°åˆ†å¸ƒå¤–åœºæ™¯ï¼Œå¹¶æ˜¾è‘—å‡å°‘å¥–åŠ±é»‘å®¢ï¼ˆReward Hackingï¼‰çš„ç°è±¡ã€‚

* **æ˜“äºå®šåˆ¶åŒ–:**  æˆ‘ä»¬æä¾›äº† POLAR çš„é¢„è®­ç»ƒæƒé‡ï¼ˆPOLAR-Baseï¼‰ã€‚ç ”ç©¶äººå‘˜å¯ä»¥æ ¹æ®è‡ªèº«éœ€æ±‚ï¼Œä¾¿æ·åœ°å¯¹å…¶è¿›è¡Œå¾®è°ƒä»¥é€‚é…å„ç§å®šåˆ¶åŒ–åœºæ™¯ã€‚

<br><img src="./assets/intro.jpeg"/><br>


# æ¨¡å‹åº“

æ­¤æ¬¡å‘å¸ƒçš„ POLAR æ¨¡å‹å‚æ•°è§„æ¨¡åˆ†åˆ«ä¸º 1.8B å’Œ 7Bã€‚**POLAR-1.8B-Base** å’Œ **POLAR-7B-Base** æ˜¯ä»…ç»è¿‡é¢„è®­ç»ƒé˜¶æ®µçš„æƒé‡ï¼Œé€‚åˆæ ¹æ®ç‰¹å®šéœ€æ±‚è¿›è¡Œå¾®è°ƒã€‚**POLAR-1.8B** å’Œ **POLAR-7B** æ˜¯ç»è¿‡åå¥½å¾®è°ƒçš„å¥–åŠ±æ¨¡å‹ï¼Œå¯å¼€ç®±å³ç”¨ï¼Œé€‚ç”¨äºå¤§éƒ¨åˆ†é€šç”¨åœºæ™¯ã€‚

| æ¨¡å‹                      | Transformers(HF)                           | ModelScope(HF)                           |
| -------------------------- | ------------------------------------------ | ---------------------------------------- |
| **POLAR-1.8B-Base**       | [ğŸ¤— POLAR-1_8B-Base](https://huggingface.co/internlm/POLAR-1_8B-Base) | [ğŸ¤– POLAR-1_8B-Base](https://modelscope.cn/models/Shanghai_AI_Laboratory/POLAR-1_8B-Base/summary) |
| **POLAR-1.8B**  | [ğŸ¤— POLAR-1_8B](https://huggingface.co/internlm/POLAR-1_8B) | [ğŸ¤– POLAR-1_8B](https://modelscope.cn/models/Shanghai_AI_Laboratory/POLAR-1_8B/summary) |
| **POLAR-7B-Base**         | [ğŸ¤— POLAR-7B-Base](https://huggingface.co/internlm/POLAR-7B-Base) | [ğŸ¤– POLAR-7B-Base](https://modelscope.cn/models/Shanghai_AI_Laboratory/POLAR-7B-Base/summary) |
| **POLAR-7B**    | [ğŸ¤— POLAR-7B](https://huggingface.co/internlm/POLAR-7B) | [ğŸ¤– POLAR-7B](https://modelscope.cn/models/Shanghai_AI_Laboratory/POLAR-7B/summary) |


# æ€§èƒ½

æˆ‘ä»¬é€šè¿‡ Proximal Policy Optimizationï¼ˆPPOï¼‰ç®—æ³•å¯¹ POLAR çš„ä½¿ç”¨æ•ˆæœè¿›è¡Œäº†éªŒè¯ï¼Œè¯„æµ‹äº†å››ç§è¯­è¨€æ¨¡å‹çš„ä¸‹æ¸¸å¼ºåŒ–å­¦ä¹ æ€§èƒ½ï¼Œè¯„æµ‹å·¥å…·æ˜¯ [OpenCompass](https://github.com/internLM/OpenCompass/) ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚é˜…[è®ºæ–‡](https://arxiv.org/abs/2507.05197)ã€‚

<img src="./assets/result.png"/><br>

# å¿«é€Ÿå¼€å§‹

## å®‰è£…

æ¨èä½¿ç”¨æœ€æ–°çš„ [xtuner](https://github.com/InternLM/xtuner) æ¥å¾®è°ƒå’Œä½¿ç”¨ POLARã€‚xtuner æ˜¯ä¸€ä¸ªé«˜æ•ˆã€çµæ´»ã€å…·æœ‰å¤šç§ä½¿ç”¨ç‰¹æ€§çš„è¯­è¨€æ¨¡å‹å¾®è°ƒå·¥å…·ã€‚

- å»ºè®®ä½¿ç”¨ conda åˆ›å»º Python-3.10 è™šæ‹Ÿç¯å¢ƒï¼š

  ```bash
  conda create --name xtuner-env python=3.10 -y
  conda activate xtuner-env
  ```

- é€šè¿‡ pip å®‰è£… xtunerï¼š

  ```shell
  pip install 'git+https://github.com/InternLM/xtuner.git@main#egg=xtuner[deepspeed]'
  ```

## æ¨ç†

æˆ‘ä»¬æ”¯æŒé€šè¿‡ [lmdeploy](https://github.com/InternLM/lmdeploy/)ã€[sglang](https://github.com/sgl-project/sglang/)ã€[vllm](https://github.com/vllm-project/vllm/) å¯¹ POLAR è¿›è¡Œæ¨ç†å¹¶è·å–å¥–åŠ±ä¿¡å·ã€‚å»ºè®®åœ¨ä½¿ç”¨è¿™äº›æ¨ç†å¼•æ“æ—¶ï¼Œåˆ›å»º conda è™šæ‹Ÿç¯å¢ƒï¼Œä»¥é¿å…å¯èƒ½å‡ºç°çš„ä¾èµ–å†²çªé—®é¢˜ã€‚

### æ•°æ®æ ¼å¼

ä¸ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹ä¸åŒï¼ŒPOLAR éœ€è¦é¢å¤–çš„å‚è€ƒç­”æ¡ˆã€‚POLAR å¯¹æ¨¡å‹è¾“å‡ºè½¨è¿¹ä¸å‚è€ƒç­”æ¡ˆçš„ä¸€è‡´æ€§è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ç»™å‡ºå¥–åŠ±åˆ†æ•°ã€‚

```python
data = [
    {
        "prompt": [{"role": "user", "content": "What is the capital of China?"}],
        "reference": [{"role": "assistant", "content": "Beijing."}],
        "output": [{"role": "assistant", "content": "Beijing."}]
    },
    {
        "prompt": [{"role": "user", "content": "What is the capital of China?"}],
        "reference": [{"role": "assistant", "content": "Beijing."}],
        "output": [{"role": "assistant", "content": "Shanghai."}]
    }
]
```

### ä½¿ç”¨ transformers è¿›è¡Œæ¨ç†

#### ç¤ºä¾‹ä»£ç 

```python
from transformers import AutoModel, AutoTokenizer
from xtuner.utils import RewardModelClient

model_name = 'internlm/POLAR-7B'

model = AutoModel.from_pretrained(
    model_name,
    device_map="cuda", 
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

client = RewardModelClient(model_name)
encoded_data = client.encode(data)
batch = tokenizer(encoded_data, return_tensors='pt', padding=True).to('cuda')
outputs = model(**batch)
rewards = outputs[0].squeeze(-1).cpu().tolist()
print(rewards)
# [-0.5702977776527405, -11.030370712280273] for previous example data
```

### ä½¿ç”¨ lmdeploy è¿›è¡Œæ¨ç†

[LMDeploy](https://github.com/InternLM/lmdeploy) æ˜¯ä¸€ä¸ªé«˜æ•ˆå‹ç¼©ã€éƒ¨ç½²è¯­è¨€æ¨¡å‹çš„å·¥å…·ã€‚

#### ç¯å¢ƒä¾èµ–

- lmdeploy >= 0.9.1

#### å¯åŠ¨æœåŠ¡ç«¯

```bash
lmdeploy serve api_server internlm/POLAR-7B --backend pytorch --server-port 30000
```
#### å®¢æˆ·ç«¯è¯·æ±‚ç¤ºä¾‹

```python
from xtuner.utils import RewardModelClient

client = RewardModelClient("internlm/POLAR-7B",
                           server_type="lmdeploy",
                           server_address="127.0.0.1:30000")

# Request rewards directly
rewards = client(data)
print(rewards)

# First encode data and then get rewards via the request function.
encoded_data = client.encode(data)
rewards = client.lmdeploy_request_reward(encoded_data)
print(rewards)
```

### ä½¿ç”¨ sglang è¿›è¡Œæ¨ç†

#### ç¯å¢ƒä¾èµ–

- 0.4.3.post4 <= sglang <= 0.4.4.post1

#### å¯åŠ¨æœåŠ¡ç«¯

```bash
python3 -m sglang.launch_server --model internlm/POLAR-7B --trust-remote-code --is-embedding --dp 4 --tp 2 --mem-fraction-static 0.9 --port 30000
```

#### å®¢æˆ·ç«¯è¯·æ±‚ç¤ºä¾‹

```python
from xtuner.utils import RewardModelClient

client = RewardModelClient("internlm/POLAR-7B",
                           server_type="sglang",
                           server_address="127.0.0.1:30000")

# Request rewards directly
rewards = client(data)
print(rewards)

# First encode data and then get rewards via the request function.
encoded_data = client.encode(data)
rewards = client.sglang_request_reward(encoded_data)
print(rewards)
```

### ä½¿ç”¨ vllm è¿›è¡Œæ¨ç†

#### ç¯å¢ƒä¾èµ–

- vllm >= 0.8.0

#### å¯åŠ¨æœåŠ¡ç«¯

```bash
vllm serve internlm/POLAR-7B --task=reward --trust-remote-code --tensor-parallel-size=2 --port 30000
```

#### å®¢æˆ·ç«¯è¯·æ±‚ç¤ºä¾‹

```python
from xtuner.utils import RewardModelClient

client = RewardModelClient("internlm/POLAR-7B",
                           server_type="vllm",
                           server_address="127.0.0.1:30000")

# Request rewards directly
rewards = client(data)
print(rewards)

# First encode data and then get rewards via the request function.
encoded_data = client.encode(data)
rewards = client.vllm_request_reward(encoded_data)
print(rewards)
```

## åå¥½å¾®è°ƒ

### ç¯å¢ƒä¾èµ–

- flash_attn
- tensorboard

### æ•°æ®æ ¼å¼

ä¸ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹ä¸åŒï¼Œé™¤äº† chosen è½¨è¿¹å’Œ rejected è½¨è¿¹ï¼ŒPOLAR åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­è¿˜éœ€è¦ä¸€ä¸ªé¢å¤–çš„å‚è€ƒç­”æ¡ˆä½œä¸ºç¤ºèŒƒã€‚ä½ å¯ä»¥é€šè¿‡æ„å»ºä¸€ä¸ª `train.jsonl` çš„æ–‡ä»¶æ¥å‡†å¤‡å¾®è°ƒæ•°æ®ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

```json
{
    "prompt": [{"role": "user", "content": "What is the capital of China?"}],
    "reference": [{"role": "assistant", "content": "Beijing."}],
    "chosen": [{"role": "assistant", "content": "Beijing."}],
    "rejected": [{"role": "assistant", "content": "Shanghai."}]
}
```

### è®­ç»ƒæ­¥éª¤

- **ç¬¬ä¸€æ­¥:** å‡†å¤‡é…ç½®æ–‡ä»¶ã€‚æˆ‘ä»¬æä¾›äº†å¯ç›´æ¥ä½¿ç”¨çš„[ç¤ºä¾‹é…ç½®](./examples/xtuner_configs/POLAR_7B_full_varlenattn_custom_dataset.py)ã€‚å¦‚æœéœ€è¦è¿›ä¸€æ­¥å¯¹è¶…å‚è¿›è¡Œä¿®æ”¹ï¼Œè¯·å¤åˆ¶ä¸€ä»½ç¤ºä¾‹é…ç½®æ–‡ä»¶ï¼Œå¹¶æ ¹æ® [xtuner ä½¿ç”¨æŒ‡å—](https://github.com/InternLM/xtuner/blob/main/docs/en/get_started/quickstart.md) è¿›è¡Œä¿®æ”¹ã€‚æœ‰å…³å¥–åŠ±æ¨¡å‹è®­ç»ƒè®¾ç½®çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ [xtuner å¥–åŠ±æ¨¡å‹](https://github.com/InternLM/xtuner/blob/main/docs/en/reward_model/modify_settings.md)ã€‚

- **ç¬¬äºŒæ­¥:** å¯åŠ¨å¾®è°ƒã€‚

    ```shell
    xtuner train ${CONFIG_FILE_PATH}
    ```

    ä¾‹å¦‚ï¼Œä½ å¯ä»¥æŒ‰ç…§å¦‚ä¸‹çš„æ–¹å¼å¾®è°ƒ POLAR-7B-Baseï¼š
  ```shell
  # On a single GPU
  xtuner train ./examples/xtuner_configs/POLAR_7B_full_varlenattn_custom_dataset.py --deepspeed deepspeed_zero2

  # On multiple GPUs
  NPROC_PER_NODE=${GPU_NUM} xtuner train ./examples/xtuner_configs/POLAR_7B_full_varlenattn_custom_dataset.py --deepspeed deepspeed_zero2
  ```

  è¿™é‡Œï¼Œ`--deepspeed` è¡¨ç¤ºä½¿ç”¨ [DeepSpeed](https://github.com/microsoft/DeepSpeed) æ¥åŠ é€Ÿè®­ç»ƒã€‚xtuner å†…ç½®äº†å¤šç§ DeepSpeed ç­–ç•¥ï¼ŒåŒ…æ‹¬ ZeRO-1ã€ZeRO-2 å’Œ ZeRO-3ã€‚å¦‚æœæ‚¨æƒ³ç¦ç”¨æ­¤åŠŸèƒ½ï¼Œåªéœ€ç§»é™¤æ­¤å‚æ•°å³å¯ã€‚

- **ç¬¬ä¸‰æ­¥:** å°†ä¿å­˜çš„ PTH æ¨¡å‹ï¼ˆè‹¥ä½¿ç”¨ DeepSpeedï¼Œåˆ™ä¿å­˜ç»“æœä¼šæ˜¯ä¸€ä¸ªç›®å½•ï¼‰è½¬æ¢ä¸º HuggingFace æ¨¡å‹ï¼Œå‘½ä»¤å¦‚ä¸‹ï¼š

  ```shell
  xtuner convert pth_to_hf ${CONFIG_FILE_PATH} ${PTH} ${SAVE_PATH}
  ```
<br>

# æ•ˆæœç¤ºä¾‹

## å®¢è§‚é—®ç­”

```python
from xtuner.utils import RewardModelClient

prompt = "å•è¯â€œstrawberryâ€ä¸­æœ‰å‡ ä¸ªâ€œrâ€ï¼Ÿ"
reference = "å•è¯â€œstrawberryâ€ä¸­åŒ…å«3ä¸ªå­—æ¯â€œrâ€ã€‚æˆ‘ä»¬å¯ä»¥é€å­—æ¯æ•°ä¸€ä¸‹ï¼šâ€œsâ€ã€â€œtâ€ã€â€œrâ€ã€â€œaâ€ã€â€œwâ€ã€â€œbâ€ã€â€œeâ€ã€â€œrâ€ã€â€œrâ€ã€â€œyâ€ã€‚å› æ­¤ï¼Œç­”æ¡ˆæ˜¯3ã€‚"
outputs = [
    # ä¸å‚è€ƒå®Œå…¨ä¸€è‡´
    "å•è¯â€œstrawberryâ€ä¸­åŒ…å«3ä¸ªå­—æ¯â€œrâ€ã€‚æˆ‘ä»¬å¯ä»¥é€å­—æ¯æ•°ä¸€ä¸‹ï¼šâ€œsâ€ã€â€œtâ€ã€â€œrâ€ã€â€œaâ€ã€â€œwâ€ã€â€œbâ€ã€â€œeâ€ã€â€œrâ€ã€â€œrâ€ã€â€œyâ€ã€‚å› æ­¤ï¼Œç­”æ¡ˆæ˜¯3ã€‚",
    # æ€è·¯æ­£ç¡®ï¼Œç­”æ¡ˆæ­£ç¡®
    "æˆ‘ä»¬æ¥æ•°ä¸€æ•°å•è¯â€œstrawberryâ€ä¸­æœ‰å‡ ä¸ªâ€œrâ€ï¼šâ€œsâ€ã€â€œtâ€ã€â€œrâ€ã€â€œaâ€ã€â€œwâ€ã€â€œbâ€ã€â€œeâ€ã€â€œrâ€ã€â€œrâ€ã€â€œyâ€ã€‚è¿™é‡Œä¸€å…±æœ‰ä¸‰ä¸ªâ€œrâ€ï¼Œå› æ­¤ç­”æ¡ˆæ˜¯ä¸‰ã€‚",
    # æ€è·¯é”™è¯¯ï¼Œç­”æ¡ˆé”™è¯¯
    "æˆ‘ä»¬æ¥æ•°ä¸€æ•°å•è¯â€œstrawberryâ€ä¸­æœ‰å‡ ä¸ªâ€œrâ€ï¼šâ€œsâ€ã€â€œtâ€ã€â€œrâ€ã€â€œaâ€ã€â€œwâ€ã€â€œbâ€ã€â€œeâ€ã€â€œrâ€ã€â€œrâ€ã€â€œyâ€ã€‚è¿™é‡Œä¸€å…±æœ‰ä¸¤ä¸ªâ€œrâ€ï¼Œå› æ­¤ç­”æ¡ˆæ˜¯äºŒã€‚",
    # æ€è·¯é”™è¯¯ï¼Œç­”æ¡ˆæ­£ç¡®
    "æˆ‘ä»¬æ¥æ•°ä¸€æ•°å•è¯â€œstrawberryâ€ä¸­æœ‰å‡ ä¸ªâ€œrâ€ï¼šâ€œsâ€ã€â€œtâ€ã€â€œrâ€ã€â€œaâ€ã€â€œwâ€ã€â€œbâ€ã€â€œeâ€ã€â€œrâ€ã€â€œrâ€ã€â€œyâ€ã€‚è¿™é‡Œä¸€å…±æœ‰ä¸¤ä¸ªâ€œrâ€ï¼Œå› æ­¤ç­”æ¡ˆæ˜¯ä¸‰ã€‚",
    # æ€è·¯æ­£ç¡®ï¼Œç­”æ¡ˆé”™è¯¯
    "æˆ‘ä»¬æ¥æ•°ä¸€æ•°å•è¯â€œstrawberryâ€ä¸­æœ‰å‡ ä¸ªâ€œrâ€ï¼šâ€œsâ€ã€â€œtâ€ã€â€œrâ€ã€â€œaâ€ã€â€œwâ€ã€â€œbâ€ã€â€œeâ€ã€â€œrâ€ã€â€œrâ€ã€â€œyâ€ã€‚è¿™é‡Œä¸€å…±æœ‰ä¸‰ä¸ªâ€œrâ€ï¼Œå› æ­¤ç­”æ¡ˆæ˜¯äºŒã€‚",
    # ç­”æ¡ˆæ­£ç¡®
    "å•è¯â€œstrawberryâ€ä¸­æœ‰3ä¸ªâ€œrâ€",
    # ç­”æ¡ˆé”™è¯¯
    "å•è¯â€œstrawberryâ€ä¸­æœ‰2ä¸ªâ€œrâ€"
]
data = [{"prompt": prompt, "reference": reference, "output": output} for output in outputs]

client = RewardModelClient("internlm/POLAR-7B", server_type="sglang", server_address="127.0.0.1:30000")
rewards = client(data)

sorted_res = sorted(zip(outputs, rewards), key=lambda x: x[1], reverse=True)

for output, reward in sorted_res:
    print(f"Output: {output}\nReward: {reward}\n")
```

```txt
Output: å•è¯â€œstrawberryâ€ä¸­åŒ…å«3ä¸ªå­—æ¯â€œrâ€ã€‚æˆ‘ä»¬å¯ä»¥é€å­—æ¯æ•°ä¸€ä¸‹ï¼šâ€œsâ€ã€â€œtâ€ã€â€œrâ€ã€â€œaâ€ã€â€œwâ€ã€â€œbâ€ã€â€œeâ€ã€â€œrâ€ã€â€œrâ€ã€â€œyâ€ã€‚å› æ­¤ï¼Œç­”æ¡ˆæ˜¯3ã€‚
Reward: -1.5380859375

Output: æˆ‘ä»¬æ¥æ•°ä¸€æ•°å•è¯â€œstrawberryâ€ä¸­æœ‰å‡ ä¸ªâ€œrâ€ï¼šâ€œsâ€ã€â€œtâ€ã€â€œrâ€ã€â€œaâ€ã€â€œwâ€ã€â€œbâ€ã€â€œeâ€ã€â€œrâ€ã€â€œrâ€ã€â€œyâ€ã€‚è¿™é‡Œä¸€å…±æœ‰ä¸‰ä¸ªâ€œrâ€ï¼Œå› æ­¤ç­”æ¡ˆæ˜¯ä¸‰ã€‚
Reward: -2.767578125

Output: å•è¯â€œstrawberryâ€ä¸­æœ‰3ä¸ªâ€œrâ€
Reward: -7.45703125

Output: æˆ‘ä»¬æ¥æ•°ä¸€æ•°å•è¯â€œstrawberryâ€ä¸­æœ‰å‡ ä¸ªâ€œrâ€ï¼šâ€œsâ€ã€â€œtâ€ã€â€œrâ€ã€â€œaâ€ã€â€œwâ€ã€â€œbâ€ã€â€œeâ€ã€â€œrâ€ã€â€œrâ€ã€â€œyâ€ã€‚è¿™é‡Œä¸€å…±æœ‰ä¸‰ä¸ªâ€œrâ€ï¼Œå› æ­¤ç­”æ¡ˆæ˜¯äºŒã€‚
Reward: -7.6328125

Output: æˆ‘ä»¬æ¥æ•°ä¸€æ•°å•è¯â€œstrawberryâ€ä¸­æœ‰å‡ ä¸ªâ€œrâ€ï¼šâ€œsâ€ã€â€œtâ€ã€â€œrâ€ã€â€œaâ€ã€â€œwâ€ã€â€œbâ€ã€â€œeâ€ã€â€œrâ€ã€â€œrâ€ã€â€œyâ€ã€‚è¿™é‡Œä¸€å…±æœ‰ä¸¤ä¸ªâ€œrâ€ï¼Œå› æ­¤ç­”æ¡ˆæ˜¯ä¸‰ã€‚
Reward: -8.65625

Output: æˆ‘ä»¬æ¥æ•°ä¸€æ•°å•è¯â€œstrawberryâ€ä¸­æœ‰å‡ ä¸ªâ€œrâ€ï¼šâ€œsâ€ã€â€œtâ€ã€â€œrâ€ã€â€œaâ€ã€â€œwâ€ã€â€œbâ€ã€â€œeâ€ã€â€œrâ€ã€â€œrâ€ã€â€œyâ€ã€‚è¿™é‡Œä¸€å…±æœ‰ä¸¤ä¸ªâ€œrâ€ï¼Œå› æ­¤ç­”æ¡ˆæ˜¯äºŒã€‚
Reward: -9.2890625

Output: å•è¯â€œstrawberryâ€ä¸­æœ‰2ä¸ªâ€œrâ€
Reward: -11.921875
```

## ä¸»è§‚é—®ç­”
```python
from xtuner.utils import RewardModelClient

prompt = "å¸®æˆ‘æƒ³3ä¸ªå½¢å®¹é›¨å¾ˆå¤§çš„æˆè¯­ï¼Œè¦æ±‚ä¸èƒ½é‡å¤ã€‚"
reference = "1. å€¾ç›†å¤§é›¨ 2. æš´é›¨å¦‚æ³¨ 3. ç“¢æ³¼å¤§é›¨"
outputs = [
    # ä¸å‚è€ƒç›¸åŒ
    "1. å€¾ç›†å¤§é›¨ 2. æš´é›¨å¦‚æ³¨ 3. ç“¢æ³¼å¤§é›¨",
    # æ­£ç¡®å›ç­”
    "1. å¤§é›¨æ»‚æ²± 2. ç‹‚é£éª¤é›¨ 3. å¤§é›¨å¦‚æ³¨",
    # éæˆè¯­
    "1. æ€¥é›¨å¦‚ç€‘ 2. è±ªé›¨å€¾å¤© 3. é›¨åŠ¿ç£…ç¤´",
    # ä¸å‚è€ƒç±»ä¼¼ï¼Œå¤šä¸€ä¸ªã€‚
    "1. å€¾ç›†å¤§é›¨ 2. æš´é›¨å¦‚æ³¨ 3. ç“¢æ³¼å¤§é›¨ 4. å¤§é›¨æ»‚æ²±",
    # ä¸å‚è€ƒç±»ä¼¼ï¼Œé‡å¤ä¸€ä¸ªã€‚
    "1. å€¾ç›†å¤§é›¨ 2. æš´é›¨å¦‚æ³¨ 3. æš´é›¨å¦‚æ³¨",
    # ä¸å‚è€ƒç±»ä¼¼ï¼Œå°‘ä¸€ä¸ªã€‚
    "1. å€¾ç›†å¤§é›¨ 2. æš´é›¨å¦‚æ³¨",
    # æˆè¯­æ­£ç¡®ï¼Œå¤šä¸€ä¸ªã€‚
    "1. å¤§é›¨æ»‚æ²± 2. ç‹‚é£éª¤é›¨ 3. å¤§é›¨å¦‚æ³¨ 4. å€¾ç›†å¤§é›¨", 
    # æˆè¯­æ­£ç¡®ï¼Œé‡å¤ä¸€ä¸ª
    "1. å¤§é›¨æ»‚æ²± 2. ç‹‚é£éª¤é›¨ 3. ç‹‚é£éª¤é›¨",
    # æˆè¯­æ­£ç¡®ï¼Œå°‘ä¸€ä¸ª
    "1. å¤§é›¨æ»‚æ²± 2. ç‹‚é£éª¤é›¨"
]
data = [{"prompt": prompt, "reference": reference, "output": output} for output in outputs]

client = RewardModelClient("internlm/POLAR-7B", server_type="sglang", server_address="127.0.0.1:30000")
rewards = client(data)

sorted_res = sorted(zip(outputs, rewards), key=lambda x: x[1], reverse=True)

for output, reward in sorted_res:
    print(f"Output: {output}\nReward: {reward}\n")
```

```txt
Output: 1. å€¾ç›†å¤§é›¨ 2. æš´é›¨å¦‚æ³¨ 3. ç“¢æ³¼å¤§é›¨
Reward: -1.42578125

Output: 1. å¤§é›¨æ»‚æ²± 2. ç‹‚é£éª¤é›¨ 3. å¤§é›¨å¦‚æ³¨
Reward: -5.234375

Output: 1. å€¾ç›†å¤§é›¨ 2. æš´é›¨å¦‚æ³¨ 3. ç“¢æ³¼å¤§é›¨ 4. å¤§é›¨æ»‚æ²±
Reward: -5.62890625

Output: 1. æ€¥é›¨å¦‚ç€‘ 2. è±ªé›¨å€¾å¤© 3. é›¨åŠ¿ç£…ç¤´
Reward: -5.7109375

Output: 1. å€¾ç›†å¤§é›¨ 2. æš´é›¨å¦‚æ³¨
Reward: -6.61328125

Output: 1. å€¾ç›†å¤§é›¨ 2. æš´é›¨å¦‚æ³¨ 3. æš´é›¨å¦‚æ³¨
Reward: -6.65234375

Output: 1. å¤§é›¨æ»‚æ²± 2. ç‹‚é£éª¤é›¨
Reward: -6.828125

Output: 1. å¤§é›¨æ»‚æ²± 2. ç‹‚é£éª¤é›¨ 3. å¤§é›¨å¦‚æ³¨ 4. å€¾ç›†å¤§é›¨
Reward: -7.0234375

Output: 1. å¤§é›¨æ»‚æ²± 2. ç‹‚é£éª¤é›¨ 3. ç‹‚é£éª¤é›¨
Reward: -7.23046875
```

# è®¸å¯è¯

ä»£ç å’Œæ¨¡å‹æƒé‡å‡é‡‡ç”¨ Apache-2.0 è®¸å¯è¯ã€‚

# å¼•ç”¨

```
@article{dou2025pretrained,
  title={Pre-Trained Policy Discriminators are General Reward Models},
  author={Dou, Shihan and Liu, Shichun and Yang, Yuming and Zou, Yicheng and Zhou, Yunhua and Xing, Shuhao and Huang, Chenhao and Ge, Qiming and Song, Demin and Lv, Haijun and others},
  journal={arXiv preprint arXiv:2507.05197},
  year={2025}
}
```
